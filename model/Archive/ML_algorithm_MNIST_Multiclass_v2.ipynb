{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "mndata = MNIST('/Users/dcusworth/Desktop/mnist/MNIST/python-mnist/data')\n",
    "images, labels = mndata.load_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build feature map\n",
    "N = 1000 #How many images I want to load\n",
    "d = 784 #Pixels of MNIST data\n",
    "    \n",
    "#label_func = lambda x,choose_label: [1 if la == choose_label else -1 for la in x]\n",
    "def label_func(x, choose_label):\n",
    "    if x == choose_label:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Retrieve data and labels - do preprocessing\n",
    "y_labs = labels[0:N]\n",
    "\n",
    "#Loop over set of regularization parameters\n",
    "vaccs = []\n",
    "lambdas = [10**q for q in np.linspace(-5,5,10)]\n",
    "\n",
    "#Load images\n",
    "feature_map = np.zeros((N,d))\n",
    "for i in range(N): #Just do a subset of training for now\n",
    "    feature_map[i,:] = images[i]\n",
    "\n",
    "#Start spark instance on points\n",
    "#Take train test split\n",
    "sinds = range(N)\n",
    "random.shuffle(sinds)\n",
    "tint = int(.8*N)\n",
    "tind = sinds[0:tint]\n",
    "vind = sinds[tint:-1]\n",
    "\n",
    "#Center - i.e. remove mean image\n",
    "fpoints = sc.parallelize(feature_map)\n",
    "fmean = fpoints.map(lambda x: x).reduce(lambda x,y: (x+y) ) / float(N)\n",
    "x_c = fpoints.map(lambda x: x-fmean).collect()\n",
    "\n",
    "#Create Spark context for feature matrix\n",
    "x_t = sc.parallelize([xx for idx,xx in enumerate(x_c) if idx in tind])\n",
    "xtb = sc.broadcast(x_t.collect())\n",
    "x_v = sc.parallelize([xx for idx,xx in enumerate(x_c) if idx in vind])\n",
    "\n",
    "start = time.time()\n",
    "for ll in lambdas:\n",
    "\n",
    "    ws = []\n",
    "    iouts = []\n",
    "    classes = []\n",
    "    \n",
    "    #Get denominator - depends on lambda/regularization and not label\n",
    "    denom_map = x_t.map(lambda x: np.dot(x, x.T) + N*ll) \n",
    "    denom_sum = denom_map.reduce(lambda x,y: x+y)\n",
    "\n",
    "    ### Loop over all labels\n",
    "    for choose_label in range(10): \n",
    "\n",
    "        #Make Spark contexts for certain label/lambda\n",
    "        y_label = [label_func(q,choose_label) for q in y_labs]\n",
    "        tpoints = sc.parallelize(zip([yy for idx,yy in enumerate(y_labs) if idx in tind], xtb.value))\n",
    "        vpoints = sc.parallelize([yy for idx,yy in enumerate(y_labs) if idx in vind])\n",
    "        y_val = vpoints.map(lambda x:x).collect()\n",
    "\n",
    "        #Analytical solution to problem for certain label\n",
    "        #Do numerator first - requires label data\n",
    "        numer_map = tpoints.map(lambda x:x[1] * (label_func(x[0],choose_label)))\n",
    "        numer_sum = numer_map.reduce(lambda x,y: x+y)\n",
    "\n",
    "        #Use previously computed denominator \n",
    "        iw = numer_sum / float(denom_sum)\n",
    "\n",
    "        #Test on validation set\n",
    "        ires = x_v.map(lambda x:np.dot(x,iw))\n",
    "        iout = ires.collect()\n",
    "        iclass = ires.map(lambda x: np.sign(x)).collect()\n",
    "\n",
    "        #Append to output  - Add MPI communication or further spark-ize\n",
    "        ws.append(iw)\n",
    "        iouts.append(iout)\n",
    "        classes.append(iclass)\n",
    "\n",
    "    #Collect all digit predictions\n",
    "    out_pred = zip(*iouts)\n",
    "\n",
    "    #Make prediction among all digits\n",
    "    preds = []\n",
    "    for idx in range(len(out_pred)):\n",
    "        ipreds = np.asarray(out_pred[idx])\n",
    "        iclass = np.where(ipreds == np.max(ipreds))[0][0] \n",
    "        preds.append(iclass)\n",
    "\n",
    "    #Determine accuracy on validation\n",
    "    vacc = np.sum([y == p for y,p in zip(y_val, preds)]) / float(len(preds))\n",
    "    \n",
    "    #Append to lambda\n",
    "    vaccs.append(vacc)\n",
    "\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy =  0.708542713568\n",
      "best lambda = 1e-05\n",
      "elapsed time for 1000 samples =  41.8670392036 seconds\n"
     ]
    }
   ],
   "source": [
    "best_val = np.where(vaccs == np.max(vaccs))[0][0]\n",
    "print 'validation accuracy = ', vaccs[best_val]\n",
    "print 'best lambda =', lambdas[best_val]\n",
    "print 'elapsed time for', N, 'samples = ', end-start, 'seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
