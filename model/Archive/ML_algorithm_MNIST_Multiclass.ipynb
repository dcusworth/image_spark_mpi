{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "mndata = MNIST('/Users/dcusworth/Desktop/mnist/MNIST/python-mnist/data')\n",
    "images, labels = mndata.load_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build feature map\n",
    "N = 5000 #How many images I want to load\n",
    "d = 784 #Pixels of MNIST data\n",
    "\n",
    "def bayes_rule(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "#label_func = lambda x,choose_label: [1 if la == choose_label else -1 for la in x]\n",
    "def label_func(x, choose_label):\n",
    "    if x == choose_label:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 219.0 failed 1 times, most recent failure: Lost task 0.0 in stage 219.0 (TID 876, localhost): java.lang.OutOfMemoryError: Java heap space\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1671)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1345)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1707)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1345)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)\n\tat java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:501)\n\tat org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply$mcV$sp(ParallelCollectionRDD.scala:74)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1160)\n\tat org.apache.spark.rdd.ParallelCollectionPartition.readObject(ParallelCollectionRDD.scala:70)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1896)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:72)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:98)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1671)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1345)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1707)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1345)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)\n\tat java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:501)\n\tat org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply$mcV$sp(ParallelCollectionRDD.scala:74)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1160)\n\tat org.apache.spark.rdd.ParallelCollectionPartition.readObject(ParallelCollectionRDD.scala:70)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1896)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:72)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:98)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e7bdbf36037d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m#Get denominator - depends on lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mdenom_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mll\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Need to add regularization - lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mdenom_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdenom_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0miw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumer_sum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenom_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \"\"\"\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 219.0 failed 1 times, most recent failure: Lost task 0.0 in stage 219.0 (TID 876, localhost): java.lang.OutOfMemoryError: Java heap space\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1671)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1345)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1707)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1345)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)\n\tat java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:501)\n\tat org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply$mcV$sp(ParallelCollectionRDD.scala:74)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1160)\n\tat org.apache.spark.rdd.ParallelCollectionPartition.readObject(ParallelCollectionRDD.scala:70)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1896)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:72)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:98)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1671)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1345)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1707)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1345)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)\n\tat java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:501)\n\tat org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply$mcV$sp(ParallelCollectionRDD.scala:74)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1160)\n\tat org.apache.spark.rdd.ParallelCollectionPartition.readObject(ParallelCollectionRDD.scala:70)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1896)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:72)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:98)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#Retrieve data and labels - do preprocessing\n",
    "y_labs = labels[0:N]\n",
    "\n",
    "#Loop over set of regularization parameters\n",
    "vaccs = []\n",
    "lambdas = [10**q for q in np.linspace(-5,5,10)]\n",
    "\n",
    "\n",
    "#Load images\n",
    "feature_map = np.zeros((N,d))\n",
    "for i in range(N): #Just do a subset of training for now\n",
    "    feature_map[i,:] = images[i]\n",
    "\n",
    "#Start spark instance on points\n",
    "#Take train test split\n",
    "sinds = range(N)\n",
    "random.shuffle(sinds)\n",
    "tint = int(.8*N)\n",
    "tind = sinds[0:tint]\n",
    "vind = sinds[tint:-1]\n",
    "\n",
    "#Center images here\n",
    "fpoints = sc.parallelize(feature_map)\n",
    "fmean = fpoints.map(lambda x: x).reduce(lambda x,y: (x+y) ) / float(N)\n",
    "x_c = fpoints.map(lambda x: x-fmean).collect()\n",
    "\n",
    "start = time.time()\n",
    "for ll in lambdas:\n",
    "\n",
    "    ws = []\n",
    "    iouts = []\n",
    "    classes = []\n",
    "\n",
    "\n",
    "    ### Loop over all labels\n",
    "    for choose_label in range(10): \n",
    "\n",
    "        #Do binary classification for certain label\n",
    "        y_label = [label_func(q,choose_label) for q in y_labs]\n",
    "        tpoints = sc.parallelize(zip([yy for idx,yy in enumerate(y_labs) if idx in tind], \\\n",
    "                                     [xx for idx,xx in enumerate(x_c) if idx in tind]))\n",
    "        vpoints = sc.parallelize(zip([yy for idx,yy in enumerate(y_labs) if idx in vind], \\\n",
    "                                     [xx for idx,xx in enumerate(x_c) if idx in vind]))\n",
    "\n",
    "        y_val = vpoints.map(lambda x:x[0]).collect()\n",
    "        #y_val = [yy for idx,yy in enumerate(y_labs)]\n",
    "\n",
    "        ###### Analytical solution to problem for certain label #######\n",
    "\n",
    "        #Do numerator first - doesn't require regularization\n",
    "        numer_map = tpoints.map(lambda x:x[1] * (label_func(x[0],choose_label))) \n",
    "        numer_sum = numer_map.reduce(lambda x,y: x+y)\n",
    "\n",
    "        #Get denominator - depends on lambda\n",
    "        denom_map = tpoints.map(lambda x: np.dot(x[1], x[1].T) + N*ll) #Need to add regularization - lambda\n",
    "        denom_sum = denom_map.reduce(lambda x,y: x+y)\n",
    "        iw = numer_sum / float(denom_sum)\n",
    "\n",
    "        #Test on validation set\n",
    "        ires = vpoints.map(lambda x:np.dot(x[1],iw))\n",
    "        iout = ires.collect()\n",
    "        iclass = ires.map(lambda x: bayes_rule(x)).collect()\n",
    "\n",
    "        #Append to output  - Add MPI communication or further spark-ize\n",
    "        ws.append(iw)\n",
    "        iouts.append(iout)\n",
    "        classes.append(iclass)\n",
    "\n",
    "\n",
    "    #Figure out how to spark-ify this loop\n",
    "    out_pred = zip(*iouts)\n",
    "\n",
    "    preds = []\n",
    "    for idx in range(len(out_pred)):\n",
    "        ipreds = np.asarray(out_pred[idx])\n",
    "        iclass = np.where(ipreds == np.max(ipreds))[0][0] \n",
    "        preds.append(iclass)\n",
    "\n",
    "    #Determine accuracy on validation\n",
    "    vacc = np.sum([y == p for y,p in zip(y_val, preds)]) / float(len(preds))\n",
    "    \n",
    "    #Append to lambda\n",
    "    vaccs.append(vacc)\n",
    "\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_val = np.where(vaccs == np.max(vaccs))[0][0]\n",
    "print 'validation accuracy = ', vaccs[best_val]\n",
    "print 'best lambda =', lambdas[best_val]\n",
    "print 'elapsed time for', N, 'samples = ', end-start, 'seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
