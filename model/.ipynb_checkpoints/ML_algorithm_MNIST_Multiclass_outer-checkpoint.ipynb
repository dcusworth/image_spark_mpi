{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "mndata = MNIST('/Users/dcusworth/Desktop/mnist/MNIST/python-mnist/data')\n",
    "images, labels = mndata.load_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build feature map\n",
    "N = 1000 #How many images I want to load\n",
    "d = 784 #Pixels of MNIST data\n",
    "    \n",
    "#label_func = lambda x,choose_label: [1 if la == choose_label else -1 for la in x]\n",
    "def label_func(x, choose_label):\n",
    "    if x == choose_label:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " validation accuracy =  0.824120603015\n",
      "best lambda = 599.484250319\n",
      "elapsed time for 1000 samples =  10.8703939915 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#Retrieve data and labels - do preprocessing\n",
    "y_labs = labels[0:N]\n",
    "\n",
    "#Loop over set of regularization parameters\n",
    "vaccs = []\n",
    "lambdas = [10**q for q in np.linspace(-5,5,10)]\n",
    "label_dat = range(10)\n",
    "\n",
    "#Load images\n",
    "feature_map = np.zeros((N,d))\n",
    "for i in range(N): #Just do a subset of training for now\n",
    "    feature_map[i,:] = images[i]\n",
    "\n",
    "#Start spark instance on points\n",
    "#Take train test split\n",
    "sinds = range(N)\n",
    "random.shuffle(sinds)\n",
    "tint = int(.8*N)\n",
    "tind = sinds[0:tint]\n",
    "vind = sinds[tint:-1]\n",
    "\n",
    "#Center - i.e. remove mean image\n",
    "fpoints = sc.parallelize(feature_map)\n",
    "fmean = fpoints.map(lambda x: x).reduce(lambda x,y: (x+y) ) / float(N)\n",
    "x_c = fpoints.map(lambda x: x-fmean).collect()\n",
    "\n",
    "#Create Spark context for feature matrix\n",
    "x_t = sc.parallelize(list(enumerate(x_c))).filter(lambda x: x[0] in tind).map(lambda x: x[1])\n",
    "xtb = sc.broadcast(x_t.collect())\n",
    "x_v = sc.parallelize(list(enumerate(x_c))).filter(lambda x: x[0] in vind).map(lambda x: x[1])\n",
    "xvb = sc.broadcast(x_v.collect())\n",
    "\n",
    "#Get training/test labels\n",
    "ytrain = sc.parallelize(list(enumerate(y_labs))).filter(lambda x: x[0] in tind).map(lambda x: x[1]).collect()\n",
    "y_val = sc.parallelize(list(enumerate(y_labs))).filter(lambda x: x[0] in vind).map(lambda x: x[1]).collect()\n",
    "tpoints = sc.parallelize(zip(ytrain, xtb.value))\n",
    "\n",
    "\n",
    "#Get pseudo-inverse\n",
    "pseudo_inv = sc.parallelize(zip(lambdas, [np.asarray(xtb.value)] * len(lambdas)))\\\n",
    "    .map(lambda x: (x[0], np.linalg.inv(np.dot(x[1].T, x[1]) + np.eye(x[1].shape[1])*N*x[0])))\\\n",
    "    .collect()\n",
    "\n",
    "#Get transformation of Y - i.e. X^T * Y\n",
    "XtY= sc.parallelize(zip(label_dat, [np.asarray(ytrain)] * len(label_dat), [np.asarray(xtb.value)] * len(label_dat)))\\\n",
    "    .map(lambda x: (x[0], [label_func(q, x[0]) for q in x[1]], x[2]))\\\n",
    "    .map(lambda x: (x[0], np.dot(x[2].T, x[1]))).collect()\n",
    "\n",
    "#Get combinations of labels and lambdas\n",
    "def solution_func(ipseudo, XtY):\n",
    "    #ipseudo = pseudo_inv[0]\n",
    "\n",
    "    iouts = sc.parallelize(XtY)\\\n",
    "            .map(lambda x: (x[0], np.dot(ipseudo[1], x[1])))\\\n",
    "            .map(lambda x: (x[0], np.dot(np.asarray(xvb.value), x[1])))\\\n",
    "            .sortByKey(True)\\\n",
    "            .map(lambda x: x[1])\\\n",
    "            .collect()\n",
    "\n",
    "    out_pred = zip(*iouts)\n",
    "    ipred = sc.parallelize(zip(*iouts)).map(lambda x: np.argmax(x)).collect()\n",
    "    \n",
    "    return (ipseudo[0], ipred)\n",
    "\n",
    "#Run over all lambdas\n",
    "sols = []\n",
    "for ipinv in pseudo_inv:\n",
    "    sols.append(solution_func(ipinv, XtY))\n",
    "\n",
    "#Find the best lambda\n",
    "best_sol = sc.parallelize(sols)\\\n",
    "    .map(lambda x: (x[0], np.sum([y == p for y,p in zip(y_val, x[1])]) / float(len(x[1]))))\\\n",
    "    .max(lambda x:x[1])\n",
    "    \n",
    "end = time.time()\n",
    "    \n",
    "print 'validation accuracy = ', best_sol[1]\n",
    "print 'best lambda =', best_sol[0]\n",
    "print 'elapsed time for', N, 'samples = ', end-start, 'seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
