{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e64d141f88df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m#Read image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0msingle_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Select only layer 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m#Make full binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/matplotlib/image.pyc\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;31m# tricky in C.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m         \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m         \u001b[0;31m# If fname is a URL, download the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/python.app/Contents/lib/python2.7/urlparse.pyc\u001b[0m in \u001b[0;36murlparse\u001b[0;34m(url, scheme, allow_fragments)\u001b[0m\n\u001b[1;32m    142\u001b[0m     (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n\u001b[1;32m    143\u001b[0m     \u001b[0mtuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fragments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfragment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muses_params\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m';'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splitparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "import findspark\n",
    "findspark.init() \n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "\n",
    "########### MAKE DATA SELECTION ###########\n",
    "which_data = 'OWN'\n",
    "###########################################\n",
    "\n",
    "###Choosing to use MNIST dataset\n",
    "if which_data == 'MNIST':\n",
    "    \n",
    "    import mnist\n",
    "    images = mnist.train_images()\n",
    "    labels = mnist.train_labels()\n",
    "\n",
    "    d = 784 #Pixels of MNIST data\n",
    "\n",
    "\n",
    "###Choosing to use our own images\n",
    "elif which_data == 'OWN':\n",
    "    \n",
    "    \n",
    "    #Reading own images\n",
    "    import matplotlib.image as mpimg\n",
    "    import glob\n",
    "    \n",
    "    #Initialize lists\n",
    "    images_in = []\n",
    "    labels_in = []\n",
    "    \n",
    "    #Read 6 classes\n",
    "    for fingers in np.arange(6):\n",
    "        list_images = glob.glob(\"/Volumes/TRANSCEND/CS205/class_\"+str(fingers)+\"/*.png\")\n",
    "        \n",
    "        for i in np.arange(len(list_images)):\n",
    "            #Read image\n",
    "            single_image = mpimg.imread(list_images[i])[:,:,0] #Select only layer 1\n",
    "            \n",
    "            #Make full binary\n",
    "            single_image[single_image<0.5] = 0\n",
    "            single_image[single_image>0.5] = 1\n",
    "            \n",
    "            #Reshape to 1D\n",
    "            single_image = list(np.reshape(single_image,[120*180]))\n",
    "            \n",
    "            #Add image to the list\n",
    "            images_in.append(single_image)\n",
    "            \n",
    "            #Add label to the list\n",
    "            labels_in.append(fingers)\n",
    "\n",
    "    #Shuffle (cause we're reading in order, that messes up selection below)\n",
    "    #We can remove this if we're using the entire database\n",
    "\n",
    "    #Initialize lists\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    #Shuffle\n",
    "    index_shuf = range(len(labels_in))\n",
    "    random.shuffle(index_shuf)\n",
    "    for i in index_shuf:\n",
    "        images.append(images_in[i])\n",
    "        labels.append(labels_in[i])\n",
    "\n",
    "    print('Images: ', np.shape(images))\n",
    "    print('Labels: ', np.shape(labels))\n",
    "\n",
    "    d = 21600 #Pixels of finger data\n",
    "\n",
    "\n",
    "#Labeler function\n",
    "def label_func(x, choose_label):\n",
    "    if x == choose_label:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "#Iterate over different sizes of the training set\n",
    "#for N in range(1000, 60000, 10000):\n",
    "for N in [1000, 11000]:\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    #Retrieve data and labels - do preprocessing\n",
    "    y_labs = labels[0:N]\n",
    "\n",
    "    #Loop over set of regularization parameters\n",
    "    vaccs = []\n",
    "    lambdas = [10**q for q in np.linspace(-5,5,10)]\n",
    "\n",
    "    #Load images\n",
    "    feature_map = np.zeros((N,d))\n",
    "    for i in range(N): #Just do a subset of training for now\n",
    "        feature_map[i,:] = images[i].reshape(d)\n",
    "\n",
    "    #Start spark instance on points\n",
    "    #Take train test split\n",
    "    sinds = range(N)\n",
    "    random.shuffle(sinds)\n",
    "    tint = int(.8*N)\n",
    "    tind = sinds[0:tint]\n",
    "    vind = sinds[tint:-1]\n",
    "\n",
    "    #Center - i.e. remove mean image\n",
    "    fpoints = sc.parallelize(feature_map)\n",
    "    fmean = fpoints.map(lambda x: x).reduce(lambda x,y: (x+y) ) / float(N)\n",
    "    x_c = fpoints.map(lambda x: x-fmean).collect()\n",
    "\n",
    "    #Create Spark context for feature matrix\n",
    "    x_t = sc.parallelize(list(enumerate(x_c))).filter(lambda x: x[0] in tind).map(lambda x: x[1])\n",
    "    xtb = sc.broadcast(x_t.collect())\n",
    "    x_v = sc.parallelize(list(enumerate(x_c))).filter(lambda x: x[0] in vind).map(lambda x: x[1])\n",
    "    xvb = sc.broadcast(x_v.collect())\n",
    "\n",
    "    #Get training/test labels\n",
    "    ytrain = sc.parallelize(list(enumerate(y_labs))).filter(lambda x: x[0] in tind).map(lambda x: x[1]).collect()\n",
    "    y_val = sc.parallelize(list(enumerate(y_labs))).filter(lambda x: x[0] in vind).map(lambda x: x[1]).collect()\n",
    "    tpoints = sc.parallelize(zip(ytrain, xtb.value))\n",
    "\n",
    "\n",
    "    for ll in lambdas:\n",
    "\n",
    "        ws = []\n",
    "        iouts = []\n",
    "        classes = []\n",
    "        \n",
    "        #Get denominator - depends on lambda/regularization and not label\n",
    "        denom_map = x_t.map(lambda x: np.dot(x, x.T) + N*ll) \n",
    "        denom_sum = denom_map.reduce(lambda x,y: x+y) \n",
    "\n",
    "        ### Loop over all labels\n",
    "        for choose_label in range(10): \n",
    "\n",
    "            #Do numerator\n",
    "            numer_sum = tpoints.map(lambda x:x[1] * (label_func(x[0],choose_label))).reduce(lambda x,y: x+y)\n",
    "\n",
    "            #Use previously computed denominator to get fitted weights \n",
    "            iw = numer_sum / float(denom_sum)\n",
    "\n",
    "            #Test on validation set\n",
    "            ires = x_v.map(lambda x:np.dot(x,iw))\n",
    "            iout = ires.collect()\n",
    "            iclass = ires.map(lambda x: np.sign(x)).collect()\n",
    "\n",
    "            #Append to output  - Add MPI communication or further spark-ize\n",
    "            ws.append(iw)\n",
    "            iouts.append(iout)\n",
    "            classes.append(iclass)\n",
    "\n",
    "        #Collect all digit predictions\n",
    "        out_pred = zip(*iouts)\n",
    "        ipred = sc.parallelize(zip(*iouts)).map(lambda x: np.argmax(x)).collect()\n",
    "        \n",
    "        #Determine accuracy on validation\n",
    "        vacc = np.sum([y == p for y,p in zip(y_val, ipred)]) / float(len(ipred))\n",
    "\n",
    "        #Append to lambda\n",
    "        vaccs.append(vacc)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    best_val = np.where(vaccs == np.max(vaccs))[0][0]\n",
    "    with open('spark_inner_own.txt', 'a') as myfile:\n",
    "        myfile.write('validation accuracy = ' + str(vaccs[best_val]))\n",
    "        myfile.write('best lambda = ' + str(lambdas[best_val]))\n",
    "        myfile.write('elapsed time for ' + str(N) + ' samples = ' + str(end-start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
