{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "import findspark\n",
    "findspark.init() \n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########### MAKE DATA SELECTION ###########\n",
    "which_data = 'MNIST'\n",
    "###########################################\n",
    "\n",
    "###Choosing to use MNIST dataset\n",
    "if which_data == 'MNIST':\n",
    "    \n",
    "    #import mnist\n",
    "    #images = mnist.train_images()\n",
    "    #labels = mnist.train_labels()\n",
    "    \n",
    "    from mnist import MNIST\n",
    "    mndata = MNIST('/Users/dcusworth/Desktop/mnist/MNIST/python-mnist/data')\n",
    "    images, labels = mndata.load_training()\n",
    "\n",
    "    d = 784 #Pixels of MNIST data\n",
    "\n",
    "\n",
    "###Choosing to use our own images\n",
    "elif which_data == 'OWN':\n",
    "    \n",
    "    \n",
    "    #Reading own images\n",
    "    import matplotlib.image as mpimg\n",
    "    import glob\n",
    "    \n",
    "    #Initialize lists\n",
    "    images_in = []\n",
    "    labels_in = []\n",
    "    \n",
    "    #Read 6 classes\n",
    "    for fingers in np.arange(6):\n",
    "        list_images = glob.glob(\"/Volumes/TRANSCEND/CS205/class_\"+str(fingers)+\"/*.png\")\n",
    "        \n",
    "        for i in np.arange(len(list_images)):\n",
    "            #Read image\n",
    "            single_image = mpimg.imread(list_images[i])[:,:,0] #Select only layer 1\n",
    "            \n",
    "            #Make full binary\n",
    "            single_image[single_image<0.5] = 0\n",
    "            single_image[single_image>0.5] = 1\n",
    "            \n",
    "            #Reshape to 1D\n",
    "            single_image = list(np.reshape(single_image,[120*180]))\n",
    "            \n",
    "            #Add image to the list\n",
    "            images_in.append(single_image)\n",
    "            \n",
    "            #Add label to the list\n",
    "            labels_in.append(fingers)\n",
    "\n",
    "    #Shuffle\n",
    "    #Initialize lists\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    #Shuffle\n",
    "    index_shuf = range(len(labels_in))\n",
    "    random.shuffle(index_shuf)\n",
    "    for i in index_shuf:\n",
    "        images.append(images_in[i])\n",
    "        labels.append(labels_in[i])\n",
    "\n",
    "    print('Images: ', np.shape(images))\n",
    "    print('Labels: ', np.shape(labels))\n",
    "\n",
    "    d = 21600 #Pixels of finger data\n",
    "\n",
    "\n",
    "#Labeler function\n",
    "def label_func(x, choose_label):\n",
    "    if x == choose_label:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "#Iterate over different sizes of the training set\n",
    "for N in range(1000, 60000, 10000):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    #Retrieve data and labels - do preprocessing\n",
    "    y_labs = labels[0:N]\n",
    "\n",
    "    #Loop over set of regularization parameters\n",
    "    vaccs = []\n",
    "    lambdas = [10**q for q in np.linspace(-5,5,10)]\n",
    "\n",
    "    #Load images\n",
    "    feature_map = np.zeros((N,d))\n",
    "    for i in range(N): #Just do a subset of training for now\n",
    "        feature_map[i,:] = images[i]\n",
    "\n",
    "    #Start spark instance on points\n",
    "    #Take train test split\n",
    "    sinds = range(N)\n",
    "    random.shuffle(sinds)\n",
    "    tint = int(.8*N)\n",
    "    tind = sinds[0:tint]\n",
    "    vind = sinds[tint:-1]\n",
    "\n",
    "    #Center - i.e. remove mean image\n",
    "    fpoints = sc.parallelize(feature_map)\n",
    "    fmean = fpoints.map(lambda x: x).reduce(lambda x,y: (x+y) ) / float(N)\n",
    "    x_c = fpoints.map(lambda x: x-fmean).collect()\n",
    "\n",
    "    #Create Spark context for feature matrix\n",
    "    x_t = sc.parallelize(list(enumerate(x_c))).filter(lambda x: x[0] in tind).map(lambda x: x[1])\n",
    "    xtb = sc.broadcast(x_t.collect())\n",
    "    X = np.asarray(xtb.value)\n",
    "    x_v = sc.parallelize(list(enumerate(x_c))).filter(lambda x: x[0] in vind).map(lambda x: x[1])\n",
    "    xvb = sc.broadcast(x_v.collect())\n",
    "\n",
    "    #Get training/test labels\n",
    "    ytrain = sc.parallelize(list(enumerate(y_labs))).filter(lambda x: x[0] in tind).map(lambda x: x[1]).collect()\n",
    "    y_val = sc.parallelize(list(enumerate(y_labs))).filter(lambda x: x[0] in vind).map(lambda x: x[1]).collect()\n",
    "    tpoints = sc.parallelize(zip(ytrain, xtb.value))\n",
    "\n",
    "\n",
    "    for ll in lambdas:\n",
    "\n",
    "        ws = []\n",
    "        iouts = []\n",
    "        classes = []\n",
    "\n",
    "        #Get denominator - depends on lambda/regularization and not label\n",
    "        denom_sum = np.linalg.inv(np.dot(X.T, X) + np.eye(d)*ll)\n",
    "\n",
    "        ### Loop over all labels\n",
    "        for choose_label in range(10): \n",
    "\n",
    "            #Do numerator\n",
    "            numer_sum = tpoints.map(lambda x:x[1] * (label_func(x[0],choose_label))).reduce(lambda x,y: x+y)\n",
    "\n",
    "            #Use previously computed denominator to get fitted weights \n",
    "            iw = sc.parallelize(denom_sum)\\\n",
    "                    .map(lambda x: np.dot(x, numer_sum))\\\n",
    "                    .collect()\n",
    "\n",
    "            #Test on validation set\n",
    "            ires = x_v.map(lambda x:np.dot(x,iw))\n",
    "            iout = ires.collect()\n",
    "            iclass = ires.map(lambda x: np.sign(x)).collect()\n",
    "\n",
    "            #Append to output  - Add MPI communication or further spark-ize\n",
    "            ws.append(iw)\n",
    "            iouts.append(iout)\n",
    "            classes.append(iclass)\n",
    "\n",
    "        #Collect all digit predictions\n",
    "        out_pred = zip(*iouts)\n",
    "        ipred = sc.parallelize(zip(*iouts)).map(lambda x: np.argmax(x)).collect()\n",
    "\n",
    "        #Determine accuracy on validation\n",
    "        vacc = np.sum([y == p for y,p in zip(y_val, ipred)]) / float(len(ipred))\n",
    "\n",
    "        #Append to lambda\n",
    "        vaccs.append(vacc)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    best_val = np.where(vaccs == np.max(vaccs))[0][0]\n",
    "    with open('spark_inner_mnist.txt', 'a') as myfile:\n",
    "        myfile.write('validation accuracy = ' + str(vaccs[best_val]) + '\\n')\n",
    "        myfile.write('best lambda = ' + str(lambdas[best_val]) + '\\n')\n",
    "        myfile.write('elapsed time for ' + str(N) + ' samples = ' + str(end-start) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
